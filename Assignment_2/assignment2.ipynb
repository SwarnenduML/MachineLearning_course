{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Naive Bayes, Text Classification and Log-Sum-Exp-Trick\n",
    "\n",
    "Only use the already imported library `numpy`. Make sure that the `liar.txt` dataset is in the same directory as the notebook.\n",
    "\n",
    "List your team members (name and immatriculation number) in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Your names here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required packages and dataset. Do not modify.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_liar_dataset():\n",
    "    import string\n",
    "    \n",
    "    with open('liar.txt', mode='r', encoding='utf-8') as f:\n",
    "        rows = [l.strip().split('\\t')[:2] for l in f]\n",
    "    \n",
    "    y, X = zip(*rows)\n",
    "    X =[x.translate(str.maketrans('', '', string.punctuation)).lower().split() for x in X]\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "\n",
    "X, y = load_liar_dataset()\n",
    "\n",
    "print('Sample:')\n",
    "print(f'{y[0]}: {X[0]}')\n",
    "print(f'{y[2]}: {X[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 2: Fake News Classification with Naive Bayes\n",
    "\n",
    "Implement a Naive Bayes classifier with Laplace smoothing to detect whether a text message is fake or real (not fake).\n",
    "\n",
    "A text message is represented by a list of string tokens as shown above.\n",
    "The classification target is binary and the two possible labels are the strings `'fake'` and `'real'`.\n",
    "\n",
    "Fill out the methods in `NaiveBayesFakeNewsClassifier` to train (`fit`) and predict (`predict`). Feel free to introduce new fields and methods based on your needs, but the methods `fit` and `predict` are required and their interface should not be changed.\n",
    "\n",
    "Hint: Try to map the text messages to word frequency vectors by counting how often each word occurs in a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from math import log\n",
    "\n",
    "# Implement your solution here.\n",
    "class NaiveBayesFakeNewsClassifier(object):\n",
    "    def __init__(self):\n",
    "        self.total_docs = None\n",
    "        self.classes = None\n",
    "        self.docs_per_class = None\n",
    "        self.vocabulary = None\n",
    "        self.vocabulary_size = None\n",
    "        self.collection_frequency = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X is a list of `n` text messages. Each text message is a list of strings with at least length one.\n",
    "        y is a list of `n` labels either the string 'Fake' or the string 'real'.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X is a list of `n` text messages. Each text message is a list of strings with at least length one.\n",
    "        The method returns a list of `n` strings, i.e. classification labels ('fake' or 'real').\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code will evaluate your classifier.\n",
    "class RealClassifier(object):\n",
    "    \"\"\"\n",
    "    This classifier is a primitive baseline, which just predicts the most common class each time.\n",
    "    Naive Bayes should definitely beat this.\n",
    "    \"\"\"\n",
    "    def fit(self, X, y): pass\n",
    "    def predict(self, X): return len(X)*['real']\n",
    "\n",
    "    \n",
    "def train_evaluate(classifier, X, y):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Apply train-test split.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2020)\n",
    "    # Inititialize and train classifier.\n",
    "    classifier.fit(X_train, y_train)\n",
    "    # Evaluate classifier on test data.\n",
    "    yhat_test = classifier.predict(X_test)\n",
    "    cmatrix = confusion_matrix(y_test, yhat_test, labels=['real', 'fake'])\n",
    "        \n",
    "    return cmatrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cmatrix, classifier_name):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.matshow(cmatrix, cmap='Greens')\n",
    "    for x in (0, 1):\n",
    "        for y in (0, 1):\n",
    "            ax.text(x, y, cmatrix[y, x])\n",
    "    ax.set_xlabel('predicted label')\n",
    "    ax.set_ylabel('true label')\n",
    "    ax.set_xticklabels(['', 'real', 'fake'])\n",
    "    ax.set_yticklabels(['', 'real', 'fake'])\n",
    "    ax.set_title(classifier_name)\n",
    "\n",
    "    \n",
    "real_classifier = RealClassifier()\n",
    "your_classifier = NaiveBayesFakeNewsClassifier()\n",
    "real_cmatrix = train_evaluate(real_classifier, X, y)\n",
    "your_cmatrix = train_evaluate(your_classifier, X, y)\n",
    "\n",
    "plot_confusion_matrix(real_cmatrix, 'RealClassifier')\n",
    "plot_confusion_matrix(your_cmatrix, 'NaiveBayesFakeNewsClassifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 3: Log Sum Exp Trick\n",
    "\n",
    "Assume you want to apply your Bayesian classifier to a very long text. For the sake of simplicity there is only a single word $x$ and two classes $c=0$ and $c=1$. So given you have a sentence $s$ of length $n$ (only composed out of the word $x$) the probability of a class is\n",
    "\n",
    "$$ p(c | s) = \\frac{p(x | c)^{n} p(c)}{\\sum_{c'}p(x | c')^{n} p(c')} $$\n",
    "\n",
    "So the probability should converge towards $1$ with increasing $n$ for the class with the higher probability of the word $p(x|c)$. Let's implement the equation as found in the textbook and see what happens if we increase $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "def posterior(n):\n",
    "    \"\"\"\n",
    "    Probability of a class given a sentence of length n.\n",
    "    \"\"\"\n",
    "    word_given_class = np.asarray([0.095, 0.096])\n",
    "    sentence_likelihood = np.power(word_given_class, int(n))\n",
    "    prior = np.ones(2) / 2\n",
    "    posterior = (sentence_likelihood * prior) / (sentence_likelihood.T @ prior)\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1e3,50), [posterior(n) for n in np.linspace(0, 1e3, 50)], label=['c=0', 'c=1'])\n",
    "plt.xlim(0, 1e3)\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('p(c)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, that is not what we expected. Do you understand what happend here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior(n=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are observing an numerical underflow, as you multiplied too many small numbers beyond the precision of floating point numbers.\n",
    "\n",
    "A common approach is to deal with probabilities in logspace (sometimes also called logits) to make the computations more stable.\n",
    "\n",
    "**Task**: Implement the posterior computation from above in logspace. (compare slide 39 \"Implementation Details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_logspace(n):\n",
    "    \"\"\"\n",
    "    Computation of the posterior (same as above) in logspace.\n",
    "    \n",
    "    Hint: use np.exp() on the return value to return a valid probability instead of logits\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1e3,50), [posterior_logspace(n) for n in np.linspace(0, 1e3, 50)], label=['c=0', 'c=1'])\n",
    "plt.xlim(0, 1e3)\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('p(c)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This did not really fix the problem, right?\n",
    "\n",
    "The problem is the expression `np.log(np.sum(np.exp(...)))` in your code, especially the sum of very small probabilities after leaving logspace with `np.exp()`. Can you improve the stability of this operation?\n",
    "\n",
    "**Task**: Implement the same computation using the so-called log-sum-exp trick (see the hint below). \n",
    "\n",
    "Hint: Try to rewrite the expression by subtracting the maximum of all logits from each individual logit. What needs to be added in the end in order to not change the result of the computation?\n",
    "\n",
    "Btw., it is forbidden to use `np.logaddexp()` in this task, since it is already implementing the trick we are looking for in this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior_logsumexp_trick(n):\n",
    "    \"\"\"\n",
    "    Task: Implement the Log-Sum-Exp-Trick\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0,1e3,50), [posterior_logsumexp_trick(n) for n in np.linspace(0, 1e3, 50)], label=['c=0', 'c=1'])\n",
    "plt.xlim(0, 1e3)\n",
    "plt.xlabel('n')\n",
    "plt.ylabel('p(c)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting computation should now be stable for all $n$ within the range of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
